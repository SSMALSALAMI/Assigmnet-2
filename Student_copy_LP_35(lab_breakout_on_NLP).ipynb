{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSMALSALAMI/Assigmnet-2/blob/main/Student_copy_LP_35(lab_breakout_on_NLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple NLP Exercises"
      ],
      "metadata": {
        "id": "-pGLXPB9z3Tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this workbook you will be given simple exercises on NLP.\n",
        "\n",
        "You have 3 SECTIONS given in this workbook on NLP\n",
        "\n",
        "SECTION 1) NLP -Based Questions on the Sentiment Analyzer"
      ],
      "metadata": {
        "id": "oR_DDaxiz84Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NKHTxOxz2zB"
      },
      "outputs": [],
      "source": [
        "# (using TextBlob)\n",
        "from textblob import TextBlob\n",
        "\n",
        "while True:\n",
        "    review = input(\"Enter a statement (or type 'exit' to quit): \")\n",
        "    if review.lower() == 'exit':\n",
        "        print(\"Exiting the sentiment analyzer.\")\n",
        "        break\n",
        "\n",
        "    analysis = TextBlob(review)\n",
        "    print(\"Polarity:\", analysis.sentiment.polarity)\n",
        "\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        print(\"Positive review\\n\")\n",
        "    elif analysis.sentiment.polarity < 0:\n",
        "        print(\"Negative review\\n\")\n",
        "    else:\n",
        "        print(\"It's a Neutral review\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add your answers here:**\n",
        "\n",
        "Execute the given code and try to improvise on this based on the below instructions.\n",
        "TASK 1:\n",
        "\n",
        "Q1. What is sentiment analysis in NLP? How does TextBlob determine sentiment?\n",
        "\n",
        "Q2. Is TextBlob a machine learning model? Why or why not? What kind of NLP techniques does it use internally?\n",
        "\n",
        "Do a quick study (you may use LLMs if needed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PIW26sU60hhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now improvise the above given code:\n",
        "\n",
        "# Task 2:\n",
        "\n",
        "#Q3. Try to add a subjectivity score & briefly explain about it\n",
        "\n",
        "# Q4. Instead of just 3 classes (positive, negative, neutral), define a more granular scale\n"
      ],
      "metadata": {
        "id": "cAhMJ6-T0ils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 2. Named Entity Recognition (NER)\n",
        "\n",
        "Goal: Extract named entities (like person, organization, location) from news headlines.\n",
        "\n",
        "Tools: spaCy"
      ],
      "metadata": {
        "id": "V5TL4hmS0zea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is opening a new campus in Austin, Texas.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "utaYFOh3054U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task for NER:\n",
        "Load multiple headlines from a file and extract named entities (like persons, locations, organizations\n",
        "\n",
        "For this you may try to use a public dataset (BBC News Summary Dataset or from Kaggle)\n",
        "\n",
        "Or\n",
        "\n",
        "Try to create your own .txt file with headlines (try to create atleast 20-30 headlines and save it as a .txt file, upload to your Colab notebook)\n",
        "\n",
        "Step 2: Read the File in Python\n",
        "\n",
        "Step 3: Extract Entities Using spaCy\n",
        "\n",
        "Step 4: Count how many times each entity type appears and plot a bar chart (try using matplotlib or seaborn).\n",
        "\n",
        "Step 5: Plot Bar Chart Using Matplotlib\n",
        "\n"
      ],
      "metadata": {
        "id": "NhGBdk3P2x0P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3ynQNps4ts1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions on  Section 2: (NER)\n",
        "1. What are the most common entity types in the headlines?\n",
        "\n",
        "2. Did you find any headlines without any named entities? Why do you think that is?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0HDB80O84uDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjmAA6cb5T8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION 3.\n",
        "Text Classification with TF-IDF\n",
        "\n",
        "Goal: Build a classifier to detect spam messages.\n",
        "\n",
        "Tools: pandas, sklearn, nltk\n",
        "\n",
        "For this task try to download the spam file.\n",
        "\n",
        "#Any spam dataset can be used.or you may use the one from https://github.com/mohitgupta-1O1/Kaggle-SMS-Spam-Collection-Dataset-/blob/master/spam.csv\n"
      ],
      "metadata": {
        "id": "3sSDE8e-5VKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (e.g., SMS Spam Collection Dataset)\n",
        "# Train a Naive Bayes classifier using TF-IDF features\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"spam.csv\", encoding='latin-1')[['v1', 'v2']]\n",
        "df.columns = ['label', 'text']\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "BH67IDsI9E45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OTHER METRICES - THE CLASSIFICATION REPORT\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n"
      ],
      "metadata": {
        "id": "U6Xd1Wcz9HkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Ham', 'Spam'])\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "Yabr71dP9Kt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow up question on Task 3:\n",
        "\n",
        "Whatâ€™s more important in spam detection: precision or recall? Why?\n",
        "\n",
        "Share your opinion."
      ],
      "metadata": {
        "id": "UViYRmKD9OmP"
      }
    }
  ]
}